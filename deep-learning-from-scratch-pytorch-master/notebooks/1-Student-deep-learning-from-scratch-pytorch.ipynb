{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Appreciate that machine learning is a technical, cultural, economic, and social discipline that has the ability to consolidate and re-arrange power structures;\n",
    "- Build simple ML models for classification and regression using `scikit-learn`;\n",
    "- Hand-code forward propagation for single and multilayer perceptrons using `numpy`;\n",
    "- Incorporate non-linearities into neural networks using activation functions;\n",
    "- Hand-code gradient descent using `numpy`;\n",
    "- Understand the basics of backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. An Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the science and art of teaching computers to \"learn\" patterns from data. In some ways, we can consider it a subdiscipline of data science, which is often sliced into\n",
    "\n",
    "* Descriptive analytics (BI, classic analytics, dashboards),\n",
    "* Predictive analytics (machine learning), and\n",
    "* Prescriptive analytics (decision science).\n",
    "\n",
    "Machine learning itself is often sliced into\n",
    "\n",
    "* Supervised learning (predicting a label: classification, or a continuous variable),\n",
    "* Unsupervised learning (pattern recognition for unlabelled data, a paradigm being clustering),\n",
    "* Reinforcement learning, in which software agents are placed in constrained environments and given “rewards” and “punishments” based on their activity (AlphaGo Zero, self-driving cars). \n",
    "\n",
    "\n",
    "This workshop is an introduction to deep learning, a powerful form of machine learning that has garnered much attention for its successes in computer vision (e.g. image recognition) and natural language processing.\n",
    "\n",
    "At the outset, we'd like to make clear that data science and machine learning are powerful technologies that can do both harm and good. As [Cathy O'Neil has said](https://www.datacamp.com/community/podcast/weapons-math-destruction), \n",
    "\n",
    "> \"data science doesn't just predict the future. It creates the future.\"\n",
    "\n",
    "For example,\n",
    "\n",
    "* [There are runaway feedback loops in “predictive policing”](https://www.smithsonianmag.com/innovation/artificial-intelligence-is-now-used-predict-crime-is-it-biased-180968337/), whereby more police are sent to neighborhoods with higher “reported & predicted crime,” resulting in more police being sent there and more reports of crime and so on.\n",
    "* Google search encodes all types of cultural and societal biases, such as racial discrimination, as investigated in Safiya Noble’s [Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/). An example of this is that, for many years, when using Google image search with the keyword “beautiful,” the results would be dominated by photos of white women. In the words of Ruha Benjamin, Associate Professor of African American Studies at Princeton University, [“race and technology are co-produced.”](https://www.ruhabenjamin.com/race-after-technology) \n",
    "* There are also interaction effects between many models deployed in society that mean they feedback into each other: those most likely to be treated unfairly by [healthcare algorithms](https://www.technologyreview.com/2019/10/25/132184/a-biased-medical-algorithm-favored-white-people-for-healthcare-programs/) are more likely to be discriminated against by models used in employment hiring flows and more likely to be targeted by predatory payday loan ads online, as detailed by Cathy O’Neil in [Weapons of Math Destruction](https://weaponsofmathdestructionbook.com/).\n",
    "\n",
    "![Title](../img/must-read-books.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Moreover, data collection and data reporting are political acts and processes embedded in societies with asymmetric power relations, and most often processes controlled by those in positions of power. In the words of Catherine D’Ignazio and Lauren F. Klein in [Data Feminism](https://mitpress.mit.edu/books/data-feminism), “governments and corporations have long employed data and statistics as management techniques to preserve and unequal status quo.” It is a revelation to realize that the etymology of the word statistics comes from the term statecraft (we discovered this fact from Chris Wiggins’ & Matt Jones’ course [data: past, present, and future](https://data-ppf.github.io/) at Columbia University) and the ability of states and governments to wield power through the control of data collection and data reporting (they decide what is collected, reported, how it is reported, and what decisions are made).\n",
    "\n",
    "Data science, ML, and AI consolidate and re-arrange power structures: they're cultural, economic, and social tools, as well as technical tools. Also: who chooses the classification scheme, the columns, the rows? Most often, it's those in positions of power. Be careful with the algorithms you build, how they're deployed, and the features that you use:\n",
    "\n",
    "* If you think race should not be a feature in your data (which it more than likely should not), then you should throw out zip code also, as it is highly correlated with race and very often encodes it;\n",
    "* If you are given a dataset with gender or biological sex as a feature, you should question why it was even collected in the first place and whether including it in your could discrimate against any gender or sex (hint: to my knowledge, it's always discriminatory against non-males, such as [here](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) and [here](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're now going to jump in and build our first machine learning model. It is the (now) famous Titanic dataset, where each row is a passenger on the Titanic and the target variable (the one you're trying to predict) is whether they survived or not. The features (the variables you use to make the prediction) include their name, the fare they paid, where they embarked, **and** their **Sex**. It is an important question whether we want to use this feature. As we're interested in building the best predictive model and *not* putting it into production to make decisions and take actions that impact lives, it may be ok but I encourage you all to interrogate this question further (it is credible that we could build a more accurate model by keeping 'Sex' as a feature as, [on the Titanic](https://www.newscientist.com/article/dn22119-sinking-the-titanic-women-and-children-first-myth/), \"the captain explicitly issued an order for women and children to be saved first\").\n",
    "\n",
    "**On terminology:**\n",
    "\n",
    "- The **target variable** is the variable you are trying to predict;\n",
    "- Other variables are known as **features** (or **predictor variables**), the features that you're using to predict the target variable).\n",
    "\n",
    "**On practice and procedure:**\n",
    "\n",
    "To build machine learning models, you require two things:\n",
    "\n",
    "- **Training data** (which the algorithms learn from) and\n",
    "- An **evaluation metric**, such as accuracy.\n",
    "\n",
    "For more on these, check out Cassie Kozyrkov's wonderful articles [Forget the robots! Here’s how AI will get you](https://towardsdatascience.com/forget-the-robots-heres-how-ai-will-get-you-b674c28d6a34) and [Machine learning — Is the emperor wearing clothes?](https://medium.com/@kozyrkov/machine-learning-is-the-emperor-wearing-clothes-928fe406fe09).\n",
    "\n",
    "Also note that the ML ingredients of *training data* and *evaluation* metric can introduce all type of biases and other problems into your ML algorithms, for example:\n",
    "\n",
    "* If your training data is biased, your model more than likely will be;\n",
    "* If you optimize solely for accuracy, what happens to groups that are under-represented in your training data?\n",
    "\n",
    "The latter challenge follows from the broader class of problems we face when optimizing anything, as detailed by Rachel Thomas in [\"The problem with metrics is a big problem for AI\"](https://www.fast.ai/2019/09/24/metrics/):\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">The problem with metrics is a big problem for AI<br>- Most AI approaches optimize metrics<br>- Any metric is just a proxy<br>- Metrics can, and will, be gamed<br>- Metrics overemphasize short-term concerns<br>- Online metrics are gathered in highly addictive environments<a href=\"https://t.co/k0J5ksw91Q\">https://t.co/k0J5ksw91Q</a> <a href=\"https://t.co/yGLUV2T2u3\">pic.twitter.com/yGLUV2T2u3</a></p>&mdash; Rachel Thomas (@math_rachel) <a href=\"https://twitter.com/math_rachel/status/1176606580264951810?ref_src=twsrc%5Etfw\">September 24, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import our dataset and begin looking at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/hugobowne/deep-learning-from-scratch-pytorch/master/data/train.csv')\n",
    "\n",
    "# View first lines of training data\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out data types\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out summary statistics\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and first models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: a huuuuuuuuge part of model building is making sure that our models generalize to  new data. Another way to think of this is that we want our models to capture the signal, not the noise or fluctuations in the training data. If it's capturing a lot of the noise, we call this _overfitting_.\n",
    "Image from [here](https://stats.stackexchange.com/questions/192007/what-measures-you-look-at-the-determine-over-fitting-in-linear-regression/192021).\n",
    "![Title](../img/fitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we don't want to look at all the data at the start! We want to *hold out* some of the data into a *test* or *hold-out* set so that we can test how well any model we build performs on it. The data remaining is called the _training_ data as that's the data we use to _train_ the model.\n",
    "\n",
    "**Key terminology:**\n",
    "\n",
    "- **Training data** is what we train our ML models on;\n",
    "- **Test data** or a **hold-out set** is what we use to gauge how well our model performs, after we train it.\n",
    "\n",
    "**Note:** there is a slightly more sophisticated alternative to a single hold-out set called *cross validation*, which we won't cover here. Feel free to check it out [here](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
    "\n",
    "To split our data into train and test sets, scikit-learn has a pretty cool utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split your data\n",
    "from sklearn.model_selection import ____\n",
    "df_train, df_test, y_train, y_test = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot of target variable\n",
    "df_train['Survived'] = ____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More people died than survived so let's make a first baseline and very naive prediction that everybody died. \n",
    "Although this is clearly a bad model, it will give us a baseline, against which to compare any future model that we build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Survived'] = 0\n",
    "# Compute accuracy of this model\n",
    "pred_diff = ____\n",
    "accuracy = ____\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So our incredibly naive, baseline model was 61.7% accurate. This means that if we build more sophisticated models, they definitely need to perform better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to build a model called a decision tree. Before doing that, we need to do a bit of data preparation and cleaning. We'll do all of this on the original dataset before the train-test split, to make sure we treat all rows the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing numerical variables\n",
    "df['Age'] = df.Age.fillna(df.Age.median())\n",
    "df['Fare'] = df.Fare.fillna(df.Fare.median())\n",
    "\n",
    "# Check out info of data\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex into a numerical feature\n",
    "df = pd.get_dummies(df, columns=['Sex'], drop_first=True)\n",
    "\n",
    "# Select columns and view head\n",
    "df = df[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','Survived']]\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    df.drop('Survived', axis=1), df[['Survived']], test_size=0.33, random_state=41, stratify=df[['Survived']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train your model. We're going to build a decision tree and what the training process actually does is figures out the optimal ways to split the tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/decision-tree-titanic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and fit to data\n",
    "clf = ____\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and store in 'Survived' column of df_test\n",
    "Y_pred = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of this model\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS-ON: fit, predict, and ML learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve as we increase the depth of the decision tree -- this is a plot which has the accuracy of the model on both the training and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a Decision Tree Classifier\n",
    "    clf = ____\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    ____\n",
    "\n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = ____\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = ____\n",
    "\n",
    "# Generate plot\n",
    "plt.title('clf: Varying depth of tree')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KEY NOTE:** You can see when the decision trees begin to overfit to the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other common form of supervised learning is **regression**, in which we're predicting a continuous variable, rather than classifying from a finite number of labels. \n",
    "\n",
    "One great aspect of the `scikit-learn` API is that the .fit/.predict paradigm generalizes to all forms of supervised learning. You're going to perform regression on the `scikit` diabetes dataset, which we'll now import and check out together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "diabetes_data = datasets.load_diabetes()\n",
    "\n",
    "#\n",
    "print(diabetes_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into predictors and target\n",
    "diabetes_X, diabetes_y = diabetes_data['data'], diabetes_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: Building a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to build a linear regression model for this dataset using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = ____\n",
    "\n",
    "# Train the model using the training sets\n",
    "____\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = ____\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for deep learning using neural networks. These are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML models inspired by biological neural networks.\n",
    "- Good for image classification, NLP, and more. Say more here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/george.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from [here](https://www.pnas.org/content/116/4/1074/tab-figures-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making predictions with neural networks, we use a procedure called **forward propagation**. When training neural networks (that is, finding the parameters, called weights), we use a procedure called **backpropogation**. To put it another way,\n",
    "\n",
    "- **forward propagation** is for prediction (.predict());\n",
    "- **backpropogation** is for training (.fit()).\n",
    "\n",
    "\n",
    "\n",
    "So let's first jump into forward propogation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Forward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example is the single layer perceptron (SLP).\n",
    "The parameters that change when we train the model are the weights.\n",
    "Image is from [here](https://deepai.org/machine-learning-glossary-and-terms/perceptron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the weighted sum, we take each input (feature) $x_i$, multiply it by the relevant weight $w_i$, and sum them all up. \n",
    "\n",
    "This is essentially a *weighted average*! Note that \n",
    "\n",
    "* if all the weights are the same $1/n$, the weighted sum _is_ the average of the features,\n",
    "* if a weight $w_i=0$, then the respective $x_i$ does not contribute at all to the weighted sum, and\n",
    "* if a weight $w_i$ is greater than a weight $w_j$, the corresponding $x_i$ contributed more to the weighted sum than $w_j$.\n",
    "\n",
    "In `numpy`, **x** and **w** will be 1D arrays. To compute the weighted sum, you can take element-wise products of these arrays and then take the sum.\n",
    "\n",
    "It's not necessary to know linear algebra here, but if you do know a little bit, you may recognize that **x** and **w** are vectors and that the weighted sum is the _dot product_ of these vectors so that the model is given by\n",
    "\n",
    "- $y = w\\cdot x + b $ (vectors).\n",
    "\n",
    "For ease of writing code, we'll use the `np.dot()` function and pass it the relevant arrays. If you'd like, I encourage you to confirm that this does produce the weighted sum, by working through several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE DATA:** We'll use a toy example of an e-commerce website. The features are \n",
    "* amount of time on website\n",
    "* number of interactions\n",
    "* number of customer support interactions\n",
    "\n",
    "The target is amount spent in a year (if it's negative, we can interpret it as refunds), and thus this is a regression challenge. Note that this type of question isn't necessarily a good use case for deep learning (as opposed to image classification), but it has the benefit of providing a simpler example for pedagogical purposes. Also note that we don't really care about the units of the features for the time being, but in a real-world case, you definitely would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One data point\n",
    "x = np.array([10, 29, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build a single layer perceptron using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights, one for each feature\n",
    "w = ____\n",
    "# Set bias\n",
    "b = 0\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: Single Layer Perceptron for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, this wass a regressor, in that it predicts a contiuous variable. Classically, single layer perceptrons were classifiers. For a classification challenge, you can threshold the output of the regressor by using a step function, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One data point\n",
    "x = np.array([10, 29, 2])\n",
    "# Set weights, one for each feature\n",
    "w = ____\n",
    "# Set bias\n",
    "b = 0\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "# Threshold the output of the regressor using a step function\n",
    "z = ____\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bonus points, you can also turn it into a logistic regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "# One data point\n",
    "x = np.array([10, 29, 2])\n",
    "# Set weights, one for each feature\n",
    "w = ____\n",
    "# Set bias\n",
    "b = 0\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "# Threshold the output of the regressor using a sigmoid function\n",
    "z = ____\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: SLP For many data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was using a SLP for a single data point. You'll now write code to generalize to multiple data points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stick with the same toy e-commerce example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 data points\n",
    "x = np.array([[10, 29, 2], [23, 3, 9], [11, 4, 3], [6, 15, 2], [15, 3, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're going to hand code an SLP regressor and classifier for these 5 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLP for regression\n",
    "# Set weights, one for each feature\n",
    "w = np.random.normal(size=3)\n",
    "# Set bias\n",
    "b = -25\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLP for classification\n",
    "# Set weights, one for each feature\n",
    "w = ____\n",
    "# Set bias\n",
    "b = 0\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "print(y)\n",
    "# Threshold the output of the regressor using a step function\n",
    "z = ____\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bonus points, you can also turn it into a logistic regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logreg\n",
    "# Set weights, one for each feature\n",
    "w = ____\n",
    "# Set bias\n",
    "b = 25\n",
    "# Compute weighted sum + bias\n",
    "y = ____\n",
    "# Threshold the output of the regressor using logreg\n",
    "z = ____\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks generally have many layers between the input and output layers. These layers are called *hidden layers*. To see how these work, let's add one layer to get a multilayer perceptron, such as in the image below! Image from [here](https://www.researchgate.net/figure/A-hypothetical-example-of-Multilayer-Perceptron-Network_fig4_303875065). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Each of the 5 node in 1st hidden layer has 4 inputs so it will have a 4 x 5 array for weights;\n",
    "* The output layer has one node and 5 inputs so will have a 5 x 1 array of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with the toy e-commerce example from above (which has 3 inputs, not 4, remember). We'll first define the data and the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[10, 29, 2]]) # generate data\n",
    "w1 = np.random.normal(size=(3, 5)) # weights for hidden layer\n",
    "w2 = np.random.normal(size=(5, 1)) # weights for output layer\n",
    "b1 = np.random.normal(size=(1, 5))\n",
    "b2 = np.random.normal(size=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll build our MLP classifier: for each layer, we'll perform the same computation as we did for the SLP above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP classifier\n",
    "# First layer\n",
    "y1 = ____ # @ is matrix multiplication (generalization of dot product above)\n",
    "print(y1)\n",
    "# Second layer\n",
    "y2 = ____\n",
    "print(y2)\n",
    "# Output thresholding\n",
    "z = ____\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* We've used a sigmoid function in the final layer. \"True perceptrons\" use a (Heaviside step) function but, generally speaking, if you use other functions, such as a sigmoid, it's still called a multilayer perceptron ([there's more about this here on wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron));\n",
    "* Similarly, \"true perceptrons\" are classifiers *but* MLPs, in the more general sense, can also be regressors;\n",
    "* To build the MLP above, we've essentially just concatenated two linear operations so we still only have a linear regression! If the problem is non-linear, this won't be much use. To deal with non-linearities, we use activation functions. Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historically, `tanh` has been a popular activation function (see below). We've also already seen sigmoid. A popular one these days is ReLU (Rectified Linear Unit), which is defined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"Computes ReLu function\"\"\"\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we won't dive into which ones to use when  in great detail but did want to highlight some common ones.\n",
    "For rules of thumb around which to use, consider the output layer:\n",
    "\n",
    "- If the output needs to be squished between 0 and 1, use classic \"sigmoid\".\n",
    "- If the output needs to be positive-only, use ReLU.\n",
    "- If the output needs to be squished between -1 and 1, use tanh.\n",
    "\n",
    "\n",
    "Let's plot these functions together, to get a sense of what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of x-axis\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "# Figure size\n",
    "plt.figure(figsize=(10,8))\n",
    "# Plot the curves\n",
    "plt.plot(x, relu(x), linewidth=4, label=\"relu\");\n",
    "plt.plot(x, sigmoid(x), linewidth=4, label=\"sigmoid\")\n",
    "plt.plot(x, np.tanh(x), linewidth=4, label=\"tanh\")\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It's pretty cool that ReLU has been so powerful in introducing non-linearities into deep learning when it itself is piecewise linear with only two linear components!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: Adding activation functions to your MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ReLU MLP now for 5 data points and one hidden layer, which has 8 nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 data points\n",
    "x = np.array([[10, 29, 2], [23, 3, 9], [11, 4, 3], [6, 15, 2], [15, 3, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate weights and biases\n",
    "w1 = np.random.normal(size=____)\n",
    "w2 = np.random.normal(size=____)\n",
    "b1 = np.random.normal(size=____)\n",
    "b2 = np.random.normal(loc = 50, size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 1st layer, including activation function\n",
    "y1 = ____\n",
    "z1 = ____\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second layer + activation\n",
    "z2 = ____\n",
    "y = ____\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a regressor with 4 hidden layers, in which each layer has 3 nodes (setting $b=0$ throughout to simplify slightly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 data points\n",
    "x = np.array([[10, 29, 2], [23, 3, 9], [11, 4, 3], [6, 15, 2], [15, 3, 3]])\n",
    "n = 4 # number of hidden layers\n",
    "# Initialize weights dictionary\n",
    "weights = {}\n",
    "for i in range(n):\n",
    "    #print(f\"weights_{i}\")\n",
    "    # Set weights for each layer\n",
    "    weights[i] = ____\n",
    "weights[n] = ____\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "y = ____ # first layer\n",
    "for i in range(n-1):\n",
    "    y = ____ # hidden layers\n",
    "y = ____  # final layer\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on representation learning:** one of the most important sub-tasks of machine learning is feature engineering. One interesting aspect of deep learning is that neural networks tend to learn features implicitly, as a result of their structure. For example, the more layers a neural network has, the more complex features it can recognise: early layers can identify edges, then then combinations of edges, then corners, the more complex features, and so on. For more on representation learning, check out [\"Representation Learning: A Review and New Perspectives\"](https://arxiv.org/abs/1206.5538) by  Bengio et al.\n",
    "\n",
    "Now we've got a handle on forward prop., let's dive into thinking about fitting/training our neural networks, backprop, and gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the algorithm used to optimize the weights of neural networks. Before jumping into backprop, let's first check out how gradient descent can be used to optimize the weights of perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to use forward propogation to make predictions, it's time to think about how to train a neural network! That is, how we determine the best model parameters. Reminder: our NN model parameters are the weights and biases.\n",
    "\n",
    "We want to minimize the difference between the target variable $y$ and the prediction made by our forward propagation algorithm. So after a forward pass, we use *gradient descent* to change the weights and then do another forward pass and see if we have improved our predictions. Image below from [here](https://www.datasciencecentral.com/profiles/blogs/alternatives-to-the-gradient-descent-algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/gradient-descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and the Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is about optimizing the weights after a round of forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../img/perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of SLP regressor forward propagation for a single data point. Let's write a function for our SLP, as we've written it out a few times already (if you do it 3+ times, write a function!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write SLP function\n",
    "def slp(x, w, b):\n",
    "    \"\"\"Computes single layer perceptron\"\"\"\n",
    "    y = ____\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this function to do one forward pass for a single data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single data point\n",
    "x = np.array([[10, 29, 2, 7]])/100\n",
    "y = 10 # target variable\n",
    "b = 0 # bias \n",
    "w = ____ # initialize weights\n",
    "# Compute SLP\n",
    "y_hat = ____\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discuss how we want to shift each weight slightly in a direction that will improve the prediction *so* we look at how bad the prediction was (prediction minus actual value), take the dot product with the relevant xs, and multiply by the learning rate (which we set and this decides how drastic the changes to the weights will be). This is essentially calculating the slope and we then move down in that direction!\n",
    "* also note that if the prediction is correct, then y_hat - y is zero and there's no change at all.\n",
    "* note that we're updating all weights simultaneously also (in contrast to backpropagation, as we'll see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's hand code one pass of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward prop\n",
    "y_hat = slp(x, w, b)\n",
    "# Set learning rate\n",
    "learning_rate = 0.1\n",
    "# The change in w\n",
    "delta_w = learning_rate*((y_hat - y) * x)\n",
    "# The change in b\n",
    "delta_b = learning_rate*(y_hat - y) \n",
    "# Change weights and bias\n",
    "w = (w + delta_w).reshape(4)\n",
    "b = b + delta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: Plot model performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn to now plot the difference between y_hat and y as we alternate between forward prop and updating the weights using gradient descent. Note that a round of forward prop and gradient descent in this case is called an _epoch_. More generally, an _epoch_ is \"one pass over the dataset\" (not just one forward prop and gradient descent run. So for example, if you have 2000 data points, and a batch size of 50 -- how many data points you use in each round of forward prop & gradient descent -- then you have 40 iterations before you hit one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define some lists to plot y_hat - y as we iterate\n",
    "y = 0.1\n",
    "n_epochs = 30\n",
    "diff = list() # initialize list of errors\n",
    "for _ in range(n_epochs):\n",
    "    # Forward prop (SLP)\n",
    "    y_hat = ____\n",
    "    # Append error to diff\n",
    "    ____\n",
    "    # Set learning rate\n",
    "    learning_rate = 0.1\n",
    "    # Change in w, b\n",
    "    delta_w = ____\n",
    "    delta_b = ____ \n",
    "    #print(delta_w)\n",
    "    # Perform gradient descent by making change to w, b\n",
    "    w = ____\n",
    "    b = ____\n",
    "plt.plot(diff);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDS ON: For many data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to perform the same but for many data points! Instead of plotting the difference (which will be an array/vector), we'll plot the dot product of the difference with itself divided by the number of data points, which is a measure of distance (this is the mean squared error, if you know that term!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data array\n",
    "x = np.array([[10, 29, 2], [23, 3, 9], [11, 4, 3], [6, 15, 2], [15, 3, 3]])/10\n",
    "w = np.random.normal(size=3) # weights\n",
    "b = 0.1 # bias\n",
    "diff = list() # initialize list of MSE\n",
    "for _ in range(10):\n",
    "    # Forward prop (SLP)\n",
    "    y_hat = ____\n",
    "    # Append MSE to diff\n",
    "    ____\n",
    "    # Set learning rate\n",
    "    learning_rate = 0.1\n",
    "    # Change in w\n",
    "    delta_w = ____\n",
    "    #print(delta_w)\n",
    "    # Perform gradient descent by making change to w\n",
    "    w = ____\n",
    "plt.plot(diff);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner to how forward prop takes the input data of features through your neural network and outputs a prediction in the output layer, backprop takes the error from your prediction and propogates it back through the network.\n",
    "\n",
    "Backprop calculates the slopes necessary to update the weights as it propagates back through the network. To calculate these slopes, we need use the chain rule from calculus, which is outside the scope of this notebook. You can find out more from Sebastian Raschka's great work [here](https://sebastianraschka.com/faq/docs/backprop-arbitrary.html).\n",
    "\n",
    "To say a little more in order to give a sense of things, what we're doing when performing backpropagation is we're approximating the slope of the error (loss) function with respect to each weight. What the chain rule tells us is that the slope (gradient) of the error with respect to a given weight is the product of:\n",
    "\n",
    "- the value of the node going into that weight,\n",
    "- the slope of the activation function at the weight's output, and\n",
    "- the slope of the error (loss) function with respect to the node it goes into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see a schematic of backprop (many thanks to [Sebastian Raschka](https://sebastianraschka.com/) for this!). The mathematics and calculus are not really the important parts at the moment, but more to understand the _flow_ of backprop through the network, in the opposite direction to forward propagation. In the next notebook, you'll see how to hand-code backprop using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch12/images/12_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../img/rasbt-backprop.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
